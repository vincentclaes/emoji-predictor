{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fine Tune CLIP on Tweets\n",
    "\n",
    "- CLIP on huggingface: https://huggingface.co/openai/clip-vit-base-patch32\n",
    "- Dataset: https://huggingface.co/datasets/AlekseyDorkin/extended_tweet_emojis/tree/main\n",
    "\n",
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# you might want to restart the kernel\n",
    "# coupling between torch and torchvision: https://pypi.org/project/torchvision/\n",
    "!pip install torchvision==0.11.1 torch==1.10.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# you might want to restart the kernel after installation is complete.\n",
    "!pip install transformers datasets pillow ipywidgets requests jupyter jupyter_client wandb --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Init Variables and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/emoji-predictor/fine-tune/emojis'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd = !pwd\n",
    "data_path = pwd[0] + \"/emojis\"\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvincentclaes\u001b[0m (\u001b[33mdrift-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/emoji-predictor/fine-tune/wandb/run-20220729_083637-2kplxreq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/drift-ai/emoji-predictor/runs/2kplxreq\" target=\"_blank\">iconic-oath-10</a></strong> to <a href=\"https://wandb.ai/drift-ai/emoji-predictor\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/drift-ai/emoji-predictor/runs/2kplxreq?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5e0f657ee0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "wandb.init(project=\"emoji-predictor\", entity=\"drift-ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Data Preprocessors and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration vincentclaes--emoji-predictor-84ee9ecf6ec78809\n",
      "Reusing dataset parquet (/root/.cache/huggingface/datasets/vincentclaes___parquet/vincentclaes--emoji-predictor-84ee9ecf6ec78809/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6ec590c34d433f9d29be2c952efbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Resize, InterpolationMode, ConvertImageDtype, CenterCrop\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from transformers import CLIPProcessor, CLIPModel, Trainer\n",
    "from transformers import default_data_collator, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Loading Data\n",
    "dataset = load_dataset(\"vincentclaes/emoji-predictor\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "column_names = train_dataset.column_names\n",
    "assert \"label\" in column_names\n",
    "assert \"text\" in column_names\n",
    "image_column = \"label\"\n",
    "caption_column = \"text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7585044e4ecd4eeca9ad1634b86ebc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f049ffe6d440dd9ffe1261c90b1332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on val dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'transform'=<function transform_images at 0x7f5dec1b6040> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "# Loading Model and Processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "config = model.config\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = processor.tokenizer\n",
    "feature_extractor = processor.feature_extractor\n",
    "\n",
    "MAX_TEXT_LENGTH=77\n",
    "IMAGE_SIZE = config.vision_config.image_size\n",
    "\n",
    "# Preprocess Text\n",
    "def tokenize_captions(examples):\n",
    "    captions = [caption for caption in examples[caption_column]]\n",
    "    text_inputs = tokenizer(captions, max_length=MAX_TEXT_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    examples[\"input_ids\"] = text_inputs.input_ids\n",
    "    examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    function=tokenize_captions,\n",
    "    batched=True,\n",
    "    remove_columns=[col for col in column_names if col != image_column],\n",
    "    num_proc=None,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    function=tokenize_captions,\n",
    "    batched=True,\n",
    "    remove_columns=[col for col in column_names if col != image_column],\n",
    "    num_proc=None,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on val dataset\",\n",
    ")\n",
    "\n",
    "\n",
    "# Preprocess Images\n",
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            # resize and then crop the image to the image_size\n",
    "            Resize([image_size], interpolation=InterpolationMode.BICUBIC),\n",
    "            CenterCrop(image_size),\n",
    "            # convert RGB to floats\n",
    "            ConvertImageDtype(torch.float),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "image_transformations = Transform(\n",
    "    IMAGE_SIZE\n",
    ")\n",
    "image_transformations = torch.jit.script(image_transformations)\n",
    "\n",
    "\n",
    "def transform_images(examples):\n",
    "    # https://pytorch.org/vision/stable/_modules/torchvision/io/image.html#ImageReadMode\n",
    "    images = [read_image(str(Path(data_path,f\"{c}.png\")), ImageReadMode.RGB) for c in examples[image_column]]\n",
    "    examples[\"pixel_values\"] = [image_transformations(image) for image in images]\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_dataset.set_transform(transform_images)\n",
    "val_dataset.set_transform(transform_images)\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, _, _, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'precision': precision,\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(output_dir=\"./checkpoints\",\n",
    "                           weight_decay=0.1,\n",
    "                           dataloader_num_workers=0,\n",
    "                           per_device_eval_batch_size=8,\n",
    "                           per_device_train_batch_size=8,\n",
    "                           num_train_epochs=4.0,\n",
    "                           warmup_steps=0,\n",
    "                           learning_rate=5e-05,\n",
    "                           report_to=\"wandb\"\n",
    "                           ),\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14944\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7472\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1889' max='7472' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1889/7472 08:03 < 23:50, 3.90 it/s, Epoch 1.01/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.855200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.778400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoints/checkpoint-500\n",
      "Configuration saved in ./checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-1000\n",
      "Configuration saved in ./checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-1500\n",
      "Configuration saved in ./checkpoints/checkpoint-1500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-1500/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = trainer.evaluate()\n",
    "# trainer.log_metrics(\"eval\", metrics)\n",
    "# trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"finetuned_from\": \"emoji-predictor\", \n",
    "    \"tasks\": \"contrastive-image-text-modeling\", \n",
    "    \"dataset\": \"AlekseyDorkin/extended_tweet_emojis\"\n",
    "}\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
