{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fine Tune CLIP on Tweets\n",
    "\n",
    "- CLIP on huggingface: https://huggingface.co/openai/clip-vit-base-patch32\n",
    "- Dataset: https://huggingface.co/datasets/AlekseyDorkin/extended_tweet_emojis/tree/main\n",
    "\n",
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might want to restart the kernel\n",
    "# coupling between torch and torchvision: https://pypi.org/project/torchvision/\n",
    "!pip install torchvision==0.11.1 torch==1.10.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# you might want to restart the kernel after installation is complete.\n",
    "!pip install transformers datasets pillow ipywidgets requests jupyter jupyter_client wandb --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Init Variables and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = !pwd\n",
    "data_path = pwd[0] + \"/emojis\"\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "wandb.init(project=\"emoji-predictor\", entity=\"drift-ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Data Preprocessors and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration AlekseyDorkin--extended_tweet_emojis-4eb99ce06d465da0\n",
      "Reusing dataset parquet (/root/.cache/huggingface/datasets/AlekseyDorkin___parquet/AlekseyDorkin--extended_tweet_emojis-4eb99ce06d465da0/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed42d0de5b1a436f9cc9684c1002561e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/AlekseyDorkin___parquet/AlekseyDorkin--extended_tweet_emojis-4eb99ce06d465da0/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8/cache-40036b762878c645.arrow and /root/.cache/huggingface/datasets/AlekseyDorkin___parquet/AlekseyDorkin--extended_tweet_emojis-4eb99ce06d465da0/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8/cache-907f565d049ab5a6.arrow\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Resize, InterpolationMode, ConvertImageDtype, CenterCrop\n",
    "from transformers import CLIPProcessor, CLIPModel, Trainer\n",
    "from transformers import default_data_collator, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from pathlib import Path\n",
    "\n",
    "# Loading Data\n",
    "dataset = load_dataset(\"AlekseyDorkin/extended_tweet_emojis\")\n",
    "train_dataset_full = dataset[\"train\"]\n",
    "SEED = 42 # use the same seed as in step 01\n",
    "TEST_SIZE = 0.1 # use the same as in step 01\n",
    "train_test_dataset = train_dataset_full.train_test_split(test_size=TEST_SIZE, seed=SEED)\n",
    "test_dataset = train_test_dataset[\"test\"]\n",
    "train_dataset = train_test_dataset[\"test\"]\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "column_names = train_dataset.column_names\n",
    "assert \"label\" in column_names\n",
    "assert \"text\" in column_names\n",
    "image_column = \"label\"\n",
    "caption_column = \"text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'emojis train datatset')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_dataset[\"label\"])  # density=False would make counts\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('emojis train datatset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /opt/conda/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.8/site-packages (from imblearn) (0.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.22.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With over-sampling methods, the number of samples in a class should be greater or equal to the original number of samples. Originally, there is 922 samples and 250 samples are asked.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-00c30942b806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msampling_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m250\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moversample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtext_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moversample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \"\"\"\n\u001b[1;32m     48\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         self.sampling_strategy_ = check_sampling_strategy(\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampling_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36mcheck_sampling_strategy\u001b[0;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         return OrderedDict(\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sampling_strategy_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         )\n\u001b[1;32m    522\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36m_sampling_strategy_dict\u001b[0;34m(sampling_strategy, y, sampling_type)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mclass_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampling_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    292\u001b[0m                     \u001b[0;34mf\"With over-sampling methods, the number\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                     \u001b[0;34mf\" of samples in a class should be greater\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: With over-sampling methods, the number of samples in a class should be greater or equal to the original number of samples. Originally, there is 922 samples and 250 samples are asked."
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "sampling_strategy = {i:250 for i in set(train_dataset[\"label\"])}\n",
    "oversample = RandomOverSampler(sampling_strategy=sampling_strategy)\n",
    "text_over, label_over = oversample.fit(X=pd.DataFrame(train_dataset[\"text\"]), y=train_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading Model and Processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "config = model.config\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = processor.tokenizer\n",
    "feature_extractor = processor.feature_extractor\n",
    "\n",
    "MAX_TEXT_LENGTH=77\n",
    "IMAGE_SIZE = config.vision_config.image_size\n",
    "\n",
    "# Preprocess Text\n",
    "def tokenize_captions(examples):\n",
    "    captions = [caption for caption in examples[caption_column]]\n",
    "    text_inputs = tokenizer(captions, max_length=MAX_TEXT_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    examples[\"input_ids\"] = text_inputs.input_ids\n",
    "    examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    function=tokenize_captions,\n",
    "    batched=True,\n",
    "    remove_columns=[col for col in column_names if col != image_column],\n",
    "    num_proc=None,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")\n",
    "\n",
    "# Preprocess Images\n",
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            # resize and then crop the image to the image_size\n",
    "            Resize([image_size], interpolation=InterpolationMode.BICUBIC),\n",
    "            CenterCrop(image_size),\n",
    "            # convert RGB to floats\n",
    "            ConvertImageDtype(torch.float),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "image_transformations = Transform(\n",
    "    IMAGE_SIZE\n",
    ")\n",
    "image_transformations = torch.jit.script(image_transformations)\n",
    "\n",
    "\n",
    "def transform_images(examples):\n",
    "    # https://pytorch.org/vision/stable/_modules/torchvision/io/image.html#ImageReadMode\n",
    "    images = [read_image(str(Path(data_path,f\"{c}.png\")), ImageReadMode.RGB) for c in examples[image_column]]\n",
    "    examples[\"pixel_values\"] = [image_transformations(image) for image in images]\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_dataset.set_transform(transform_images)\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(output_dir=\"./checkpoints\",\n",
    "                           weight_decay=0.1,\n",
    "                           dataloader_num_workers=0,\n",
    "                           per_device_eval_batch_size=64,\n",
    "                           per_device_train_batch_size=64,\n",
    "                           num_train_epochs=3.0,\n",
    "                           warmup_steps=0,\n",
    "                           learning_rate=5e-05,\n",
    "                           report_to=\"wandb\"\n",
    "                           ),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"finetuned_from\": \"emoji-predictor\", \n",
    "    \"tasks\": \"contrastive-image-text-modeling\", \n",
    "    \"dataset\": \"AlekseyDorkin/extended_tweet_emojis\"\n",
    "}\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
