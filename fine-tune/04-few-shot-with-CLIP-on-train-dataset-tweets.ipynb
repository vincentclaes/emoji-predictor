{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Few Shot CLIP on Tweets to Predict Emoji's\n",
    "\n",
    "<!-- - Fetch the preprocessed dataset. The dataset contains tweets as text and the label is an emoji.\n",
    "- Fine tune CLIP on our dataset.\n",
    "- Push the model to Huggingface Hub. -->\n",
    "<!-- \n",
    "Pretrained CLIP model: https://huggingface.co/openai/clip-vit-base-patch32\n",
    "Got inspiration for finteuning here: https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text\n",
    " -->\n",
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "# you might want to restart the kernel\n",
    "# coupling between torch and torchvision: https://pypi.org/project/torchvision/\n",
    "get_ipython().system('pip install torchvision==0.11.1 torch==1.10.0 --quiet')\n",
    "get_ipython().system('pip install transformers datasets pillow ipywidgets requests jupyter jupyter_client wandb sklearn --upgrade --quiet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Init Variables and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvincentclaes\u001b[0m (\u001b[33mdrift-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/emoji-predictor/fine-tune/wandb/run-20220917_114937-2ohddifk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/drift-ai/emoji-predictor/runs/2ohddifk\" target=\"_blank\">divine-brook-49</a></strong> to <a href=\"https://wandb.ai/drift-ai/emoji-predictor\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/drift-ai/emoji-predictor/runs/2ohddifk?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe51dbfaf70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
    "wandb.init(project=\"emoji-predictor\", entity=\"drift-ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 2,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration vincentclaes--emoji-predictor-b9f08890f6dc005c\n",
      "Reusing dataset parquet (/root/.cache/huggingface/datasets/vincentclaes___parquet/vincentclaes--emoji-predictor-b9f08890f6dc005c/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017116546630859375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131b89dac6b142a299bf74edd7e02732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset = load_dataset(\"vincentclaes/emoji-predictor\")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "\n",
    "# code to take a sample for testing purposes\n",
    "# train_dataset = dataset[\"train\"].select(range(32))\n",
    "# val_dataset = dataset[\"validation\"].select(range(32))\n",
    "\n",
    "# test_dataset = dataset[\"test\"]\n",
    "\n",
    "column_names = train_dataset.column_names\n",
    "assert \"label\" in column_names\n",
    "assert \"text\" in column_names\n",
    "image_column = \"label\"\n",
    "caption_column = \"text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Few Shot Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I want to knowðŸ–‘: How do YOU create a life that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Would you mind writing me a reference later fo...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I want to knowðŸ–‘: How do YOU create a life that...      0\n",
       "1  Would you mind writing me a reference later fo...     16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = train_dataset.to_pandas()\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max no of rows: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1000: Dataset({\n",
       "     features: ['text', 'label', '__index_level_0__'],\n",
       "     num_rows: 9762\n",
       " })}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(df):\n",
    "    df[\"label\"].value_counts().plot(kind = 'bar')\n",
    "\n",
    "    \n",
    "def under_sample(df, max_no_rows=None):\n",
    "    if max_no_rows is None:\n",
    "        max_no_rows = min([x[1].shape[0] for x in df.groupby(\"label\")])\n",
    "    print(f\"max no of rows: {max_no_rows}\")\n",
    "    \n",
    "    def _under_sample(x):\n",
    "        # do not undersample if we have less than\n",
    "        if x.shape[0] > max_no_rows:\n",
    "            return x.sample(n=max_no_rows, replace=False, axis=0).reset_index(drop=True)\n",
    "        return x\n",
    "    \n",
    "    df = df.groupby(\"label\", as_index=False).apply(lambda x: _under_sample(x))\n",
    "    plot(df)\n",
    "    return df\n",
    "\n",
    "# _range = [1, 5, 10, 15, 20, 25, 50, 100, 250, 500]\n",
    "_range = [1000]\n",
    "\n",
    "df_train = train_dataset.to_pandas()\n",
    "dataset_dict = {}\n",
    "\n",
    "for i in _range:\n",
    "    df_train_few_shot = under_sample(df=df_train, max_no_rows=i)\n",
    "    dataset_dict[i] =  Dataset.from_pandas(df_train_few_shot)\n",
    "\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/f330785f9a2435f10d525312e9c0408703b80cfa/config.json\n",
      "text_config_dict is None. Initializing the CLIPTextConfig with default values.\n",
      "vision_config_dict is None. initializing the CLIPVisionConfig with default values.\n",
      "Model config CLIPConfig {\n",
      "  \"_commit_hash\": \"f330785f9a2435f10d525312e9c0408703b80cfa\",\n",
      "  \"_name_or_path\": \"openai/clip-vit-base-patch32\",\n",
      "  \"architectures\": [\n",
      "    \"CLIPModel\"\n",
      "  ],\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"logit_scale_init_value\": 2.6592,\n",
      "  \"model_type\": \"clip\",\n",
      "  \"projection_dim\": 512,\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": null,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": 0,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"dropout\": 0.0,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"quick_gelu\",\n",
      "    \"hidden_size\": 512,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 2048,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 77,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"clip_text_model\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 8,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 1,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.22.1\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"vocab_size\": 49408\n",
      "  },\n",
      "  \"text_config_dict\": null,\n",
      "  \"transformers_version\": null,\n",
      "  \"vision_config\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": null,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"dropout\": 0.0,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"quick_gelu\",\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 224,\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 32,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.22.1\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false\n",
      "  },\n",
      "  \"vision_config_dict\": null\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/f330785f9a2435f10d525312e9c0408703b80cfa/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing CLIPModel.\n",
      "\n",
      "All the weights of CLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use CLIPModel for predictions without further training.\n",
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/f330785f9a2435f10d525312e9c0408703b80cfa/preprocessor_config.json\n",
      "Feature extractor CLIPFeatureExtractor {\n",
      "  \"crop_size\": 224,\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/f330785f9a2435f10d525312e9c0408703b80cfa/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/f330785f9a2435f10d525312e9c0408703b80cfa/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/f330785f9a2435f10d525312e9c0408703b80cfa/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/f330785f9a2435f10d525312e9c0408703b80cfa/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/f330785f9a2435f10d525312e9c0408703b80cfa/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/f330785f9a2435f10d525312e9c0408703b80cfa/config.json\n",
      "text_config_dict is None. Initializing the CLIPTextConfig with default values.\n",
      "vision_config_dict is None. initializing the CLIPVisionConfig with default values.\n",
      "Model config CLIPConfig {\n",
      "  \"_commit_hash\": \"f330785f9a2435f10d525312e9c0408703b80cfa\",\n",
      "  \"_name_or_path\": \"openai/clip-vit-base-patch32\",\n",
      "  \"architectures\": [\n",
      "    \"CLIPModel\"\n",
      "  ],\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"logit_scale_init_value\": 2.6592,\n",
      "  \"model_type\": \"clip\",\n",
      "  \"projection_dim\": 512,\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": null,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": 0,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"dropout\": 0.0,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"quick_gelu\",\n",
      "    \"hidden_size\": 512,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 2048,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 77,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"clip_text_model\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 8,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 1,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.22.1\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"vocab_size\": 49408\n",
      "  },\n",
      "  \"text_config_dict\": null,\n",
      "  \"transformers_version\": null,\n",
      "  \"vision_config\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": null,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"dropout\": 0.0,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"quick_gelu\",\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 224,\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 32,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.22.1\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false\n",
      "  },\n",
      "  \"vision_config_dict\": null\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016659975051879883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running tokenizer on train dataset",
       "rate": null,
       "total": 10,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a7de3ef49c4140bb52d52f48763de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01795649528503418,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running tokenizer on val dataset",
       "rate": null,
       "total": 3,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070c821e8b94418f85190da39587d8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on val dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CLIPModel.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `CLIPModel.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9762\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 306\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 03:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>2.489600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoints-1000/checkpoint-306\n",
      "Configuration saved in ./checkpoints-1000/checkpoint-306/config.json\n",
      "Model weights saved in ./checkpoints-1000/checkpoint-306/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints-1000/checkpoint-306/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints-1000/checkpoint-306/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints-1000/checkpoint-306/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for few_shots, train_dataset in dataset_dict.items():\n",
    "\n",
    "    # fetch a freshc validation dataset\n",
    "    val_dataset = dataset[\"validation\"]\n",
    "\n",
    "    # Load Pretrained Model and Processor.\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    config = model.config\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    tokenizer = processor.tokenizer\n",
    "    feature_extractor = processor.feature_extractor\n",
    "\n",
    "    MAX_TEXT_LENGTH = 77\n",
    "    IMAGE_SIZE = config.vision_config.image_size\n",
    "    \n",
    "    # Process the Tweets.\n",
    "    def tokenize_captions(examples):\n",
    "        captions = [caption for caption in examples[caption_column]]\n",
    "        text_inputs = tokenizer(captions, max_length=MAX_TEXT_LENGTH, padding=\"max_length\", truncation=True)\n",
    "        examples[\"input_ids\"] = text_inputs.input_ids\n",
    "        examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "        return examples\n",
    "\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        function=tokenize_captions,\n",
    "        batched=True,\n",
    "        remove_columns=[col for col in column_names if col != image_column],\n",
    "        num_proc=None,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "    )\n",
    "\n",
    "    val_dataset = val_dataset.map(\n",
    "        function=tokenize_captions,\n",
    "        batched=True,\n",
    "        remove_columns=[col for col in column_names if col != image_column],\n",
    "        num_proc=None,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on val dataset\",\n",
    "    )\n",
    "\n",
    "#     test_dataset = test_dataset.map(\n",
    "#         function=tokenize_captions,\n",
    "#         batched=True,\n",
    "#         remove_columns=[col for col in column_names if col != image_column],\n",
    "#         num_proc=None,\n",
    "#         load_from_cache_file=False,\n",
    "#         desc=\"Running tokenizer on test dataset\",\n",
    "#     )\n",
    "    \n",
    "    # Process the Emoji images.\n",
    "    from PIL import Image\n",
    "\n",
    "    def transform_images(examples):\n",
    "        # https://pytorch.org/vision/stable/_modules/torchvision/io/image.html#ImageReadMode\n",
    "        images = [Image.open(str(Path(\"./emojis\",f\"{c}.png\"))) for c in examples[image_column]]\n",
    "        images_transformed = processor.feature_extractor(images, return_tensors=\"pt\")\n",
    "        examples[\"pixel_values\"] = images_transformed[\"pixel_values\"]\n",
    "        return examples\n",
    "\n",
    "\n",
    "    train_dataset.set_transform(transform_images)\n",
    "    val_dataset.set_transform(transform_images)\n",
    "#     test_dataset.set_transform(transform_images)\n",
    "\n",
    "\n",
    "    def collate_fn(examples):\n",
    "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "        input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "        attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"return_loss\": True,\n",
    "        }\n",
    "\n",
    "    \n",
    "    from datasets import load_metric\n",
    "    metric = load_metric(\"precision\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    from pathlib import Path\n",
    "    dir_ = f\"./checkpoints-{few_shots}\"\n",
    "    Path(dir_).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(output_dir=dir_,\n",
    "                               dataloader_num_workers=0,\n",
    "                               # no of emojis\n",
    "                               per_device_eval_batch_size=32,\n",
    "                               per_device_train_batch_size=32,\n",
    "                               num_train_epochs=1,\n",
    "    # I couldn't make evaluation work.\n",
    "    #                            evaluation_strategy = \"steps\",\n",
    "    #                            eval_steps=8,\n",
    "                               warmup_steps=0,\n",
    "                               learning_rate=5e-05,\n",
    "                               weight_decay=0.1,\n",
    "                               report_to=\"wandb\",\n",
    "                               logging_strategy=\"epoch\",\n",
    "#                                logging_steps=100,\n",
    "                               save_strategy=\"epoch\"\n",
    "                               ),\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=train_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=collate_fn,\n",
    "        tokenizer=processor\n",
    "    )\n",
    "    \n",
    "    train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,py",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
