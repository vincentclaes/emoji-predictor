{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fine Tune CLIP on Tweets to Predict Emoji's\n",
    "\n",
    "- Fetch the preprocessed dataset. The dataset contains tweets as text and the label is an emoji.\n",
    "- Fine tune CLIP on our dataset.\n",
    "- Push the model to Huggingface Hub.\n",
    "\n",
    "Pretrained CLIP model: https://huggingface.co/openai/clip-vit-base-patch32\n",
    "Got inspiration for finteuning here: https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text\n",
    "\n",
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "# you might want to restart the kernel\n",
    "# coupling between torch and torchvision: https://pypi.org/project/torchvision/\n",
    "get_ipython().system('pip install torchvision==0.11.1 torch==1.10.0 --quiet')\n",
    "get_ipython().system('pip install transformers datasets pillow ipywidgets requests jupyter jupyter_client wandb sklearn --upgrade --quiet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Init Variables and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvincentclaes\u001b[0m (\u001b[33mdrift-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/emoji-predictor/fine-tune/wandb/run-20220906_121418-2kq06sp1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/drift-ai/emoji-predictor/runs/2kq06sp1\" target=\"_blank\">fiery-planet-42</a></strong> to <a href=\"https://wandb.ai/drift-ai/emoji-predictor\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/drift-ai/emoji-predictor/runs/2kq06sp1?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbf7aff2550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
    "wandb.init(project=\"emoji-predictor\", entity=\"drift-ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 2,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration vincentclaes--emoji-predictor-84ee9ecf6ec78809\n",
      "Reusing dataset parquet (/root/.cache/huggingface/datasets/vincentclaes___parquet/vincentclaes--emoji-predictor-84ee9ecf6ec78809/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019620656967163086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a4930a3eb148c489ae3dfe6663cab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"vincentclaes/emoji-predictor\")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "\n",
    "# code to take a sample for testing purposes\n",
    "# train_dataset = dataset[\"train\"].select(range(32))\n",
    "# val_dataset = dataset[\"validation\"].select(range(32))\n",
    "\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "column_names = train_dataset.column_names\n",
    "assert \"label\" in column_names\n",
    "assert \"text\" in column_names\n",
    "image_column = \"label\"\n",
    "caption_column = \"text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Load Pretrained Model and Processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "config = model.config\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = processor.tokenizer\n",
    "feature_extractor = processor.feature_extractor\n",
    "\n",
    "MAX_TEXT_LENGTH = 77\n",
    "IMAGE_SIZE = config.vision_config.image_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Process the Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016181230545043945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running tokenizer on train dataset",
       "rate": null,
       "total": 15,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72377759c1a741739926b837511df621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01496744155883789,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running tokenizer on val dataset",
       "rate": null,
       "total": 3,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40deefe4057247998cd7022e1f85b085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on val dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015464544296264648,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running tokenizer on test dataset",
       "rate": null,
       "total": 4,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda7c2af7c704a29baa4fe394ff8dfdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on test dataset:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_captions(examples):\n",
    "    captions = [caption for caption in examples[caption_column]]\n",
    "    text_inputs = tokenizer(captions, max_length=MAX_TEXT_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    examples[\"input_ids\"] = text_inputs.input_ids\n",
    "    examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    function=tokenize_captions,\n",
    "    batched=True,\n",
    "    remove_columns=[col for col in column_names if col != image_column],\n",
    "    num_proc=None,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    function=tokenize_captions,\n",
    "    batched=True,\n",
    "    remove_columns=[col for col in column_names if col != image_column],\n",
    "    num_proc=None,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on val dataset\",\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    function=tokenize_captions,\n",
    "    batched=True,\n",
    "    remove_columns=[col for col in column_names if col != image_column],\n",
    "    num_proc=None,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on test dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. Process the Emoji images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def transform_images(examples):\n",
    "    # https://pytorch.org/vision/stable/_modules/torchvision/io/image.html#ImageReadMode\n",
    "    images = [Image.open(str(Path(\"./emojis\",f\"{c}.png\"))) for c in examples[image_column]]\n",
    "    images_transformed = processor.feature_extractor(images, return_tensors=\"pt\")\n",
    "    examples[\"pixel_values\"] = images_transformed[\"pixel_values\"]\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_dataset.set_transform(transform_images)\n",
    "val_dataset.set_transform(transform_images)\n",
    "test_dataset.set_transform(transform_images)\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7. Fine Tune CLIP on Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"precision\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(output_dir=\"./checkpoints\",\n",
    "                           dataloader_num_workers=0,\n",
    "                           per_device_eval_batch_size=16,\n",
    "                           per_device_train_batch_size=16,\n",
    "                           num_train_epochs=10,\n",
    "# I couldn't make evaluation work.\n",
    "#                            evaluation_strategy = \"steps\",\n",
    "#                            eval_steps=8,\n",
    "                           warmup_steps=0,\n",
    "                           learning_rate=5e-05,\n",
    "                           weight_decay=0.1,\n",
    "                           report_to=\"wandb\",\n",
    "                           ),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14944\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9340\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9340' max='9340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9340/9340 1:04:35, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.456600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.929600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.810900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.467300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.308900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.071700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.961100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.774700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.709700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.613500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.584200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.543600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.502400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.480600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoints/checkpoint-500\n",
      "Configuration saved in ./checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-500/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-500/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-1000\n",
      "Configuration saved in ./checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-1000/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-1500\n",
      "Configuration saved in ./checkpoints/checkpoint-1500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-1500/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-1500/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-1500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-2000\n",
      "Configuration saved in ./checkpoints/checkpoint-2000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-2000/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-2000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-2500\n",
      "Configuration saved in ./checkpoints/checkpoint-2500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-2500/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-2500/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-2500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-3000\n",
      "Configuration saved in ./checkpoints/checkpoint-3000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-3000/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-3000/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-3000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-3500\n",
      "Configuration saved in ./checkpoints/checkpoint-3500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-3500/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-3500/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-3500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-4000\n",
      "Configuration saved in ./checkpoints/checkpoint-4000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-4000/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-4000/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-4000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-4500\n",
      "Configuration saved in ./checkpoints/checkpoint-4500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-4500/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-4500/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-4500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-5000\n",
      "Configuration saved in ./checkpoints/checkpoint-5000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-5000/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-5000/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-5000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-5500\n",
      "Configuration saved in ./checkpoints/checkpoint-5500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-5500/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-5500/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-5500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-6000\n",
      "Configuration saved in ./checkpoints/checkpoint-6000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-6000/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-6000/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-6000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-6500\n",
      "Configuration saved in ./checkpoints/checkpoint-6500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-6500/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-6500/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-6500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-7000\n",
      "Configuration saved in ./checkpoints/checkpoint-7000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-7000/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-7000/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-7000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-7500\n",
      "Configuration saved in ./checkpoints/checkpoint-7500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-7500/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-7500/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-7500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-8000\n",
      "Configuration saved in ./checkpoints/checkpoint-8000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-8000/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-8000/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-8000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-8500\n",
      "Configuration saved in ./checkpoints/checkpoint-8500/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-8500/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-8500/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-8500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./checkpoints/checkpoint-9000\n",
      "Configuration saved in ./checkpoints/checkpoint-9000/config.json\n",
      "Model weights saved in ./checkpoints/checkpoint-9000/pytorch_model.bin\n",
      "Feature extractor saved in ./checkpoints/checkpoint-9000/preprocessor_config.json\n",
      "tokenizer config file saved in ./checkpoints/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoints/checkpoint-9000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_runtime': 3876.1128,\n",
       " 'train_samples_per_second': 38.554,\n",
       " 'train_steps_per_second': 2.41,\n",
       " 'total_flos': 8692476176039040.0,\n",
       " 'train_loss': 1.0506252068268411,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Not working for now :(\n",
    "# trainer.evaluate(ignore_keys=[\"text_model_output\", \"vision_model_output\", \"text_embeds\", \"logits_per_image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push Fine Tuned Model to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m--2022-09-06 15:12:08--  https://github.com/git-lfs/git-lfs/releases/download/v2.9.0/git-lfs-linux-amd64-v2.9.0.tar.gz\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/13021798/aad0ae00-f0f4-11e9-9c4b-102d589ea506?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220906%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220906T151208Z&X-Amz-Expires=300&X-Amz-Signature=dc2213b256cf4250665903ea11a85121f7eb4635891a02d2cf8f70c6bb0383ab&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=13021798&response-content-disposition=attachment%3B%20filename%3Dgit-lfs-linux-amd64-v2.9.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2022-09-06 15:12:08--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/13021798/aad0ae00-f0f4-11e9-9c4b-102d589ea506?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220906%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220906T151208Z&X-Amz-Expires=300&X-Amz-Signature=dc2213b256cf4250665903ea11a85121f7eb4635891a02d2cf8f70c6bb0383ab&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=13021798&response-content-disposition=attachment%3B%20filename%3Dgit-lfs-linux-amd64-v2.9.0.tar.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4300104 (4.1M) [application/octet-stream]\n",
      "Saving to: ‘/root/git-lfs-linux-amd64-v2.9.0.tar.gz.2’\n",
      "\n",
      "git-lfs-linux-amd64 100%[===================>]   4.10M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2022-09-06 15:12:08 (57.7 MB/s) - ‘/root/git-lfs-linux-amd64-v2.9.0.tar.gz.2’ saved [4300104/4300104]\n",
      "\n",
      "install: omitting directory 'gitlfs'\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub --quiet\n",
    "!wget https://github.com/git-lfs/git-lfs/releases/download/v2.9.0/git-lfs-linux-amd64-v2.9.0.tar.gz -P ~/ && cd ~/ && tar --no-same-owner -xf git-lfs-linux-amd64-v2.9.0.tar.gz && ./install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file checkpoints/checkpoint-9000/config.json\n",
      "text_config_dict is None. Initializing the CLIPTextConfig with default values.\n",
      "vision_config_dict is None. initializing the CLIPVisionConfig with default values.\n",
      "Model config CLIPConfig {\n",
      "  \"_name_or_path\": \"openai/clip-vit-base-patch32\",\n",
      "  \"architectures\": [\n",
      "    \"CLIPModel\"\n",
      "  ],\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"logit_scale_init_value\": 2.6592,\n",
      "  \"model_type\": \"clip\",\n",
      "  \"projection_dim\": 512,\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": null,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": 0,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"dropout\": 0.0,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"quick_gelu\",\n",
      "    \"hidden_size\": 512,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 2048,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 77,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"clip_text_model\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 8,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 1,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.21.3\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"vocab_size\": 49408\n",
      "  },\n",
      "  \"text_config_dict\": null,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": null,\n",
      "  \"vision_config\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": null,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"dropout\": 0.0,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"quick_gelu\",\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 224,\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 32,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.21.3\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false\n",
      "  },\n",
      "  \"vision_config_dict\": null\n",
      "}\n",
      "\n",
      "loading weights file checkpoints/checkpoint-9000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing CLIPModel.\n",
      "\n",
      "All the weights of CLIPModel were initialized from the model checkpoint at checkpoints/checkpoint-9000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use CLIPModel for predictions without further training.\n",
      "loading feature extractor configuration file checkpoints/checkpoint-9000/preprocessor_config.json\n",
      "Feature extractor CLIPFeatureExtractor {\n",
      "  \"crop_size\": 224,\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"processor_class\": \"CLIPProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "Didn't find file checkpoints/checkpoint-9000/added_tokens.json. We won't load it.\n",
      "loading file checkpoints/checkpoint-9000/vocab.json\n",
      "loading file checkpoints/checkpoint-9000/merges.txt\n",
      "loading file checkpoints/checkpoint-9000/tokenizer.json\n",
      "loading file None\n",
      "loading file checkpoints/checkpoint-9000/special_tokens_map.json\n",
      "loading file checkpoints/checkpoint-9000/tokenizer_config.json\n",
      "/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'create_repo': pass token='emoji-predictor' as keyword args. From version 0.12 passing these as positional arguments will result in an error,\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/huggingface_hub/hf_api.py:681: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
      "  warnings.warn(\n",
      "Cloning https://huggingface.co/vincentclaes/emoji-predictor into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020487070083618164,
       "initial": 16384,
       "n": 16384,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Download file pytorch_model.bin",
       "rate": null,
       "total": 605239073,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f071a7b57c1423584ae455529f154af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file pytorch_model.bin:   0%|          | 16.0k/577M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016921520233154297,
       "initial": 1024,
       "n": 1024,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Clean file pytorch_model.bin",
       "rate": null,
       "total": 605239073,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f9cb137c4a47e0846f9782c422093c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/577M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/tmpsrt6vuym/config.json\n",
      "Model weights saved in /tmp/tmpsrt6vuym/pytorch_model.bin\n",
      "/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'create_repo': pass token='emoji-predictor' as keyword args. From version 0.12 passing these as positional arguments will result in an error,\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/huggingface_hub/hf_api.py:681: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
      "  warnings.warn(\n",
      "Cloning https://huggingface.co/vincentclaes/emoji-predictor into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01706242561340332,
       "initial": 32374,
       "n": 32374,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Download file pytorch_model.bin",
       "rate": null,
       "total": 605239073,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbf559b1c4149f39ea8767e92a9cd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file pytorch_model.bin:   0%|          | 31.6k/577M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016962766647338867,
       "initial": 1024,
       "n": 1024,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Clean file pytorch_model.bin",
       "rate": null,
       "total": 605239073,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3825cc5a7ebb4a8ab6a07b3fde395e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/577M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extractor saved in /tmp/tmphyxmqygp/preprocessor_config.json\n",
      "tokenizer config file saved in /tmp/tmphyxmqygp/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmphyxmqygp/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "checkpoint = \"checkpoints/checkpoint-9000\"\n",
    "model = CLIPModel.from_pretrained(checkpoint)\n",
    "processor = CLIPProcessor.from_pretrained(checkpoint)\n",
    "model.push_to_hub(\"vincentclaes/emoji-predictor\", use_temp_dir=True)\n",
    "processor.push_to_hub(\"vincentclaes/emoji-predictor\", use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "formats": "ipynb,py",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
